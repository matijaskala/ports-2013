From 57cd0958b153accf12f535ab9406dc8511bf22ec Mon Sep 17 00:00:00 2001
From: Viktor Gal <viktor.gal@maeth.com>
Date: Wed, 18 May 2016 06:35:28 +0200
Subject: [PATCH] Fixing eigen 3.3 related errors

porting fix for #3141 from lisitsyn/tapkee@7c74473d12809e4122527b6e003c74a942d8a25c
fix #3140: provide a workaround for the eigen bug for calculating log of mapped matrices
---
 src/shogun/lib/tapkee/defines.hpp                  |  4 +--
 src/shogun/mathematics/eigen3.h                    |  7 +++++
 .../logdet/opfunc/DenseMatrixExactLog.cpp          |  5 ++++
 .../SerialComputationEngine_unittest.cc            |  5 ++++
 .../linalg/DenseExactLogJob_unittest.cc            |  5 ++++
 .../linalg/RationalApproximation_unittest.cc       | 10 ++++++++
 .../linalg/SparseMatrixOperator_unittest.cc        | 30 ++++++++++++----------
 7 files changed, 51 insertions(+), 15 deletions(-)

diff --git a/src/shogun/lib/tapkee/defines.hpp b/src/shogun/lib/tapkee/defines.hpp
index fd02636..1be45fe 100644
--- a/src/shogun/lib/tapkee/defines.hpp
+++ b/src/shogun/lib/tapkee/defines.hpp
@@ -49,12 +49,12 @@ namespace tapkee
 		TapkeeOutput(const tapkee::DenseMatrix& e, const tapkee::ProjectingFunction& p) :
 			embedding(), projection(p)
 		{
-			embedding.swap(e);
+			embedding = e;
 		}
 		TapkeeOutput(const TapkeeOutput& that) :
 			embedding(), projection(that.projection)
 		{
-			this->embedding.swap(that.embedding);
+			this->embedding = that.embedding;
 		}
 		tapkee::DenseMatrix embedding;
 		tapkee::ProjectingFunction projection;
diff --git a/src/shogun/mathematics/eigen3.h b/src/shogun/mathematics/eigen3.h
index 0fb8522..734be6c 100644
--- a/src/shogun/mathematics/eigen3.h
+++ b/src/shogun/mathematics/eigen3.h
@@ -61,6 +61,13 @@
 
 	#endif	//EIGEN_VERSION_AT_LEAST(3,0,93)
 
+#if ((EIGEN_WORLD_VERSION == 3) && (EIGEN_MAJOR_VERSION == 2) && \
+	((EIGEN_MINOR_VERSION == 91) || (EIGEN_MINOR_VERSION == 92)))
+	// Regression has been introduced to eigen develop (3.3alpha1+):
+	// http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1229
+	// until this is not fixed we need to copy the matrix and calculate the log
+	#define EIGEN_WITH_LOG_BUG_1229 1
+#endif
 namespace shogun
 {
 template<class T> class SGSparseMatrix;
diff --git a/src/shogun/mathematics/linalg/ratapprox/logdet/opfunc/DenseMatrixExactLog.cpp b/src/shogun/mathematics/linalg/ratapprox/logdet/opfunc/DenseMatrixExactLog.cpp
index a7918e6..1002a6c 100644
--- a/src/shogun/mathematics/linalg/ratapprox/logdet/opfunc/DenseMatrixExactLog.cpp
+++ b/src/shogun/mathematics/linalg/ratapprox/logdet/opfunc/DenseMatrixExactLog.cpp
@@ -62,7 +62,12 @@ void CDenseMatrixExactLog::precompute()
 	Map<MatrixXd> mat(m.matrix, m.num_rows, m.num_cols);
 	SGMatrix<float64_t> log_m(m.num_rows, m.num_cols);
 	Map<MatrixXd> log_mat(log_m.matrix, log_m.num_rows, log_m.num_cols);
+#if EIGEN_WITH_LOG_BUG_1229
+	MatrixXd tmp = mat;
+	log_mat=tmp.log();
+#else
 	log_mat=mat.log();
+#endif
 
 	// the log(C) is also a linear operator here
 	// reset the operator of this function with log(C)
diff --git a/tests/unit/lib/computation/SerialComputationEngine_unittest.cc b/tests/unit/lib/computation/SerialComputationEngine_unittest.cc
index c41a69a..7f96df1 100644
--- a/tests/unit/lib/computation/SerialComputationEngine_unittest.cc
+++ b/tests/unit/lib/computation/SerialComputationEngine_unittest.cc
@@ -40,7 +40,12 @@ TEST(SerialComputationEngine, dense_log_det)
 	mat(1,1)=3.0;
 	Map<MatrixXd> m(mat.matrix, mat.num_rows, mat.num_cols);
 	Map<MatrixXd> log_m(log_mat.matrix, log_mat.num_rows, log_mat.num_cols);
+#if EIGEN_WITH_LOG_BUG_1229
+	MatrixXd tmp = m;
+	log_m=tmp.log();
+#else
 	log_m=m.log();
+#endif
 
 	// create linear operator and aggregator
 	CDenseMatrixOperator<float64_t>* log_op=new CDenseMatrixOperator<float64_t>(log_mat);
diff --git a/tests/unit/mathematics/linalg/DenseExactLogJob_unittest.cc b/tests/unit/mathematics/linalg/DenseExactLogJob_unittest.cc
index a5a12cf..60daf40 100644
--- a/tests/unit/mathematics/linalg/DenseExactLogJob_unittest.cc
+++ b/tests/unit/mathematics/linalg/DenseExactLogJob_unittest.cc
@@ -38,7 +38,12 @@ TEST(DenseExactLogJob, log_det)
 	mat(1,1)=3.0;
 	Map<MatrixXd> m(mat.matrix, mat.num_rows, mat.num_cols);
 	Map<MatrixXd> log_m(log_mat.matrix, log_mat.num_rows, log_mat.num_cols);
+#if EIGEN_WITH_LOG_BUG_1229
+	MatrixXd tmp = m;
+	log_m=tmp.log();
+#else
 	log_m=m.log();
+#endif
 
 	// create linear operator and aggregator
 	CDenseMatrixOperator<float64_t>* log_op=new CDenseMatrixOperator<float64_t>(log_mat);
diff --git a/tests/unit/mathematics/linalg/RationalApproximation_unittest.cc b/tests/unit/mathematics/linalg/RationalApproximation_unittest.cc
index f401d06..682ed66 100644
--- a/tests/unit/mathematics/linalg/RationalApproximation_unittest.cc
+++ b/tests/unit/mathematics/linalg/RationalApproximation_unittest.cc
@@ -182,7 +182,12 @@ TEST(RationalApproximation, trace_accuracy)
 #if EIGEN_VERSION_AT_LEAST(3,1,0)
 	// compute the trace of log(m) using Eigen3 that uses Schur-Parlett algorithm
 	Map<MatrixXd> eig_m(m.matrix, m.num_rows, m.num_cols);
+#if EIGEN_WITH_LOG_BUG_1229
+	MatrixXd tmp = eig_m;
+	float64_t trace_log_m=tmp.log().diagonal().sum();
+#else
 	float64_t trace_log_m=eig_m.log().diagonal().sum();
+#endif
 #else
 	float64_t trace_log_m=-11.51292546497021618279;
 #endif // EIGEN_VERSION_AT_LEAST(3,1,0)
@@ -364,7 +369,12 @@ TEST(RationalApproximation, trace_accuracy_cg_m)
 #if EIGEN_VERSION_AT_LEAST(3,1,0)
 	// compute the trace of log(m) using Eigen3 that uses Schur-Parlett algorithm
 	Map<MatrixXd> eig_m(m.matrix, m.num_rows, m.num_cols);
+#if EIGEN_WITH_LOG_BUG_1229
+	MatrixXd tmp = eig_m;
+	float64_t trace_log_m=tmp.log().diagonal().sum();
+#else
 	float64_t trace_log_m=eig_m.log().diagonal().sum();
+#endif
 #else
 	float64_t trace_log_m=-11.51292546497021618279;
 #endif // EIGEN_VERSION_AT_LEAST(3,1,0)
diff --git a/tests/unit/mathematics/linalg/SparseMatrixOperator_unittest.cc b/tests/unit/mathematics/linalg/SparseMatrixOperator_unittest.cc
index 4d30724..9d171cc 100644
--- a/tests/unit/mathematics/linalg/SparseMatrixOperator_unittest.cc
+++ b/tests/unit/mathematics/linalg/SparseMatrixOperator_unittest.cc
@@ -219,33 +219,37 @@ TEST(SparseMatrixOperator, get_set_diagonal_realloc_complex128)
 
 TEST(SparseMatrixOperator, get_sparsity_structure)
 {
-	const int size=9;
-	const int max_pow=10;
+	const int32_t size=9;
+	const int32_t max_pow=10;
 
-	SGMatrix<double> m(size, size);
+	SGMatrix<float64_t> m(size, size);
 
 	m.set_const(0.0);
-	for (int i=0; i<size; ++i)
+	for (int32_t i=0; i<size; ++i)
 		m(i,i)=2.0;
-	for (int i=0; i<size; i+=4)
+	for (int32_t i=0; i<size; i+=4)
 		m(i,size-1)=2.0;
-	for (int i=0; i<size; i+=4)
+	for (int32_t i=0; i<size; i+=4)
 		m(size-1,i)=2.0;
 
-	CSparseFeatures<double> feat(m);
-	SGSparseMatrix<double> sm=feat.get_sparse_feature_matrix();
-	CSparseMatrixOperator<double> op(sm);
+	CSparseFeatures<float64_t> feat(m);
+	SGSparseMatrix<float64_t> sm=feat.get_sparse_feature_matrix();
+	CSparseMatrixOperator<float64_t> op(sm);
 	CSparseMatrixOperator<bool>* b_op
 		=static_cast<CSparseMatrixOperator<bool>*>(op);
 
-	SparseMatrix<bool, RowMajor, int> sp
+	SparseMatrix<bool, RowMajor, int32_t> sp
 		=EigenSparseUtil<bool>::toEigenSparse(b_op->get_matrix_operator());
-	SparseMatrix<double, RowMajor, int> sm2
-		=EigenSparseUtil<double>::toEigenSparse(sm);
+	SparseMatrix<float64_t, RowMajor, int32_t> sm2
+		=EigenSparseUtil<float64_t>::toEigenSparse(sm);
 
 	// compute direct matrix power and then the sparsity structure
-	for (int i=2; i<=max_pow; ++i)
+	for (int32_t i=2; i<=max_pow; ++i)
+#if EIGEN_VERSION_AT_LEAST(3,2,91)
+		sp=(sp.cast<float64_t>()*sm2).cast<bool>();
+#else
 		sp=sp*sm2;
+#endif
 
 	int32_t* outerIndexPtr=const_cast<int32_t*>(sp.outerIndexPtr());
 	int32_t* innerIndexPtr=const_cast<int32_t*>(sp.innerIndexPtr());
